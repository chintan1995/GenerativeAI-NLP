{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "udUBTNuQXOpZ",
        "Logc48CDXirQ",
        "qBSwg7bOYCD8",
        "BAAiFNyoawX1",
        "amB5Se8cs3zo",
        "GASSH4KiuVWd"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Llama 2/OpenAI+ Pinecone + LangChain**"
      ],
      "metadata": {
        "id": "7I2wTiMSu3PB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 1: Install All the Required Pakages**"
      ],
      "metadata": {
        "id": "udUBTNuQXOpZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aEfiWhkWBHt"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain\n",
        "!pip -q install pypdf\n",
        "!pip -q install unstructured\n",
        "!pip -q install sentence_transformers\n",
        "!pip -q install pinecone-client\n",
        "!pip -q install huggingface_hub\n",
        "!pip -q install openai\n",
        "!pip -q install tiktoken\n",
        "!pip -q install bitsandbytes accelerate xformers einops\n",
        "!pip -q install datasets loralib sentencepiece\n",
        "!pip -q install chromadb\n",
        "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 2: Import All the Required Libraries**"
      ],
      "metadata": {
        "id": "Logc48CDXirQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.vectorstores import Pinecone\n",
        "import pinecone\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "from langchain.llms import LlamaCpp, OpenAI\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.prompts.chat import SystemMessagePromptTemplate\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "kmdLCsZPXqwF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc5e4a60-6e26-47ae-d7d9-8fc33033c519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 3: Load the Data**"
      ],
      "metadata": {
        "id": "qBSwg7bOYCD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/drive/MyDrive/Colab Notebooks/GenAI LLM Projects/docs\" /content/"
      ],
      "metadata": {
        "id": "wA-LV1MOtO3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document=[]\n",
        "for file in os.listdir(\"docs\"):\n",
        "  if file.endswith(\".pdf\"):\n",
        "    pdf_path=\"./docs/\"+file\n",
        "    loader=PyPDFLoader(pdf_path)\n",
        "    document.extend(loader.load())\n",
        "  elif file.endswith('.docx') or file.endswith('.doc'):\n",
        "    doc_path=\"./docs/\"+file\n",
        "    loader=Docx2txtLoader(doc_path)\n",
        "    document.extend(loader.load())\n",
        "  elif file.endswith('.txt'):\n",
        "    text_path=\"./docs/\"+file\n",
        "    loader=TextLoader(text_path)\n",
        "    document.extend(loader.load())"
      ],
      "metadata": {
        "id": "4BNql54aD_Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 4: Split the Text into Chunks**"
      ],
      "metadata": {
        "id": "68n73kgaYcl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=100)"
      ],
      "metadata": {
        "id": "Tz3q1oCgYPye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitted_docs=text_splitter.split_documents(document)"
      ],
      "metadata": {
        "id": "9kqT_CZVYr1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(splitted_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ejpwUSmYx27",
        "outputId": "1f8099c0-7378-4d83-bb45-05eca103ce9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 5: Setup the Environment. Pinecone and HfHub API keys**"
      ],
      "metadata": {
        "id": "K24-STW9ZGnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_auth = ''   # Your HuggingFace key\n",
        "pinecone_auth = ''   # Your Pinecone key\n",
        "openai_auth = ''   # Your openai key\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_auth\n",
        "\n",
        "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', pinecone_auth)\n",
        "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'gcp-starter')\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]=openai_auth"
      ],
      "metadata": {
        "id": "aZMYGbDlY1pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 6: Downlaod the Embeddings**"
      ],
      "metadata": {
        "id": "waQtomfxZhM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "embeddings_size = len(embeddings.embed_query('c'))   # 384"
      ],
      "metadata": {
        "id": "_937693LZpoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embeddings = OpenAIEmbeddings()\n",
        "# embeddings_size = len(embeddings.embed_query('c'))   # 1536"
      ],
      "metadata": {
        "id": "QMGEHP1N5gNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 8: Pinecone - Create Embeddings for Each of the Text Chunk**"
      ],
      "metadata": {
        "id": "BAAiFNyoawX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # initialize pinecone\n",
        "# pinecone.init(\n",
        "#     api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
        "#     environment=PINECONE_API_ENV  # next to api key in console\n",
        "# )"
      ],
      "metadata": {
        "id": "7vYf3XVZZ0eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# index_name = \"new-index-pc\" # put in the name of your pinecone index here\n",
        "# pinecone.delete_index(index_name)\n",
        "# pinecone.create_index(index_name, dimension=embeddings_size, metric=\"dotproduct\")"
      ],
      "metadata": {
        "id": "9h6CIg-XprJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # pinecone_db = Pinecone.from_texts([t.page_content for t in splitted_docs], embeddings, index_name=index_name)\n",
        "# pinecone_db = Pinecone.from_documents(splitted_docs, embeddings, index_name=index_name)"
      ],
      "metadata": {
        "id": "aPTbec-ea1r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## If you already have an index, you can load it like this\n"
      ],
      "metadata": {
        "id": "amB5Se8cs3zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pinecone_db = Pinecone.from_existing_index(index_name, embeddings)"
      ],
      "metadata": {
        "id": "ewMdzQFGs16I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 8: Chroma - Create Embeddings for Each of the Text Chunk**"
      ],
      "metadata": {
        "id": "yOurc3Cr_GOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./chroma_data"
      ],
      "metadata": {
        "id": "1i-FytbETJZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_db=Chroma.from_documents(splitted_docs, embedding=embeddings, persist_directory='./chroma_data')\n",
        "chroma_db.persist()"
      ],
      "metadata": {
        "id": "bU6iw4TJ_FT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 9: Similarity Search**"
      ],
      "metadata": {
        "id": "mQbKwQZubUI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_docs=chroma_db.similarity_search(\"what is attention mechanism?\")\n",
        "tmp_docs"
      ],
      "metadata": {
        "id": "T1SApOx4D8yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 9: Query the Docs to get the Answer Back (Llama 2 Model)**"
      ],
      "metadata": {
        "id": "SlcdYpG2rlRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Quantized Llama CPP from the Hugging Face Community"
      ],
      "metadata": {
        "id": "GASSH4KiuVWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ### From: https://huggingface.co/TheBloke/Llama-2-13B-GGUF/tree/main\n",
        "# model_name_or_path = \"TheBloke/Llama-2-13B-GGUF\"\n",
        "# model_basename = \"llama-2-13b.Q5_K_S.gguf\"\n",
        "# model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "\n",
        "\n",
        "# n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
        "# n_batch = 256  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "\n",
        "# # Loading model,\n",
        "# llm = LlamaCpp(\n",
        "#     model_path=model_path,\n",
        "#     max_tokens=512,\n",
        "#     n_gpu_layers=n_gpu_layers,\n",
        "#     n_batch=n_batch,\n",
        "#     callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), # Callbacks support token-wise streaming\n",
        "#     n_ctx=1024,\n",
        "#     # verbose=False,\n",
        "# )"
      ],
      "metadata": {
        "id": "ZKjMEiLNUO7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Official Meta Llama 2 Model from the Hugging Face"
      ],
      "metadata": {
        "id": "dbe2YpCWIZI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Quantization config\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "09E2cYa-WtlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Model config\n",
        "\n",
        "model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
        "\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ],
      "metadata": {
        "id": "5PUnAsN5W0xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id,\n",
        "                                          use_auth_token=hf_auth,)\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             device_map='auto',\n",
        "                                             config=model_config,\n",
        "                                             quantization_config=bnb_config,\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             use_auth_token=hf_auth,\n",
        "                                            #   load_in_8bit=True,\n",
        "                                            #   load_in_4bit=True\n",
        "                                             )"
      ],
      "metadata": {
        "id": "XG16ysqnIdls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama_pipe = pipeline(task=\"text-generation\",\n",
        "              model=model,\n",
        "              tokenizer=tokenizer,\n",
        "              torch_dtype=torch.bfloat16,\n",
        "              device_map='auto',\n",
        "              max_new_tokens=512,\n",
        "              min_new_tokens=-1,\n",
        "            #   top_k=30,   ### Top k tokens of the output tokens\n",
        "              top_p=0.1,   ### Top p cumulative probabilities of the output tokens\n",
        "            #   repetition_penalty=1.2\n",
        "              )"
      ],
      "metadata": {
        "id": "cGjg1RpKIjFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.llms import HuggingFaceHub\n",
        "# llm=HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512})\n",
        "\n",
        "# llm=HuggingFacePipeline(pipeline=llama_pipe, model_kwargs={'temperature':0})\n",
        "llm=OpenAI(temperature=0.2, model_name='gpt-3.5-turbo')   ### Default model is 'gpt-3.5-turbo'\n",
        "llm"
      ],
      "metadata": {
        "id": "1-EmKHuLIkut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query"
      ],
      "metadata": {
        "id": "_30MB2jPId5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_chars_in_docs(docs):\n",
        "    char_counter=0\n",
        "    for i in range(len(docs)):\n",
        "        char_counter+=len(docs[i].page_content)\n",
        "\n",
        "    return char_counter\n",
        "\n",
        "def reduce_retrived_tokens(retrived_docs):\n",
        "    char_count_og = count_chars_in_docs(retrived_docs)\n",
        "    max_char_limit = 3000\n",
        "    if char_count_og>max_char_limit:\n",
        "        extra_count = char_count_og - max_char_limit\n",
        "        chars_to_strip = extra_count//len(retrived_docs)\n",
        "        chars_to_strip = ((chars_to_strip//10)+1)*10   # Rounding the integer up to nearest 10's, eg, 10,20,30,etc. Eg: 57-->60\n",
        "        for i in range(len(retrived_docs)):\n",
        "            retrived_docs[i].page_content = retrived_docs[i].page_content[:-chars_to_strip]\n",
        "        char_count_new = count_chars_in_docs(retrived_docs)\n",
        "        print(\"Reduced token size...\")\n",
        "        print(f\"Original size = {char_count_og}. After reduction = {char_count_new}\")\n",
        "    else:\n",
        "        print(\"No need for token reduction...\")\n",
        "        print(f\"Original size = {char_count_og}\")\n",
        "    return retrived_docs\n",
        "\n",
        "def get_similar_docs(query, db):\n",
        "    retrived_docs=db.similarity_search(query)\n",
        "    retrived_docs=reduce_retrived_tokens(retrived_docs)\n",
        "    return retrived_docs"
      ],
      "metadata": {
        "id": "qKgb6QRrzm2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"You are a helpful assistant who gives answers in short. \\\n",
        "                     Remember you are talking to a technical audience so you can be as detailed as possible. \\\n",
        "                     Strike a friendly and converstional tone. \\\n",
        "                     Use the following pieces of context to answer the question at the end. \\\n",
        "                     If you don't know the answer, just say that you don't know it, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer in short in Hindi:\n",
        "\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "1EVFCGgqThUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is attention mechanism?\"\n",
        "retrived_docs = get_similar_docs(query=query, db=chroma_db)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNWmM3eaEDi3",
        "outputId": "cb57bb1a-075f-4fc2-9081-2dbcd21c1bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No need for token reduction...\n",
            "Original size = 2573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)"
      ],
      "metadata": {
        "id": "7cAfNG1EyTBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reply = chain.run(input_documents=retrived_docs, question=query)"
      ],
      "metadata": {
        "id": "kT9AT15XGIC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reply"
      ],
      "metadata": {
        "id": "BqWgnQabW6yy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}